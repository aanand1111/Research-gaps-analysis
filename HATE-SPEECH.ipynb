{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49166a59-a6f8-4584-94ee-ec30e95306ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6cd9f-97db-40dc-934d-8d0c24f3fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('HateSpeechDataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae010d4c-f904-431f-a127-13884128e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the text\n",
    "def preprocess_text(Content):\n",
    "    # Remove URLs and mentions\n",
    "    Content = re.sub(r'http\\S+', '', Content)\n",
    "    Content = re.sub(r'@\\w+', '', Content)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(Content.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    Content = ' '.join(tokens)\n",
    "    \n",
    "    return Content\n",
    "\n",
    "# Preprocess the text in the dataset\n",
    "df['Content'] = df['Content'].apply(preprocess_Content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45cb7a-7f92-413b-852a-c62921eb935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load a multilingual hate speech dataset\n",
    "def load_multilingual_dataset(language):\n",
    "    # Load the dataset for the specified language\n",
    "    df_lang = pd.read_csv(f'HateSpeechDataset_{language}.csv')\n",
    "    \n",
    "    # Preprocess the text in the dataset\n",
    "    df_lang['Content'] = df_lang['Content'].apply(preprocess_text)\n",
    "    \n",
    "    return df_lang\n",
    "\n",
    "# Load the multilingual hate speech datasets\n",
    "df_es = load_multilingual_dataset('es')\n",
    "df_fr = load_multilingual_dataset('fr')\n",
    "df_de = load_multilingual_dataset('de')\n",
    "\n",
    "# Concatenate the datasets\n",
    "df_all = pd.concat([df, df_es, df_fr, df_de])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_all = df_all.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(df_all))\n",
    "train_df = df_all[:train_size]\n",
    "test_df = df_all[train_size:]\n",
    "\n",
    "# Define the input and output columns\n",
    "X = train_df['Content']\n",
    "y = train_df['Label']\n",
    "\n",
    "# Define the model architecture\n",
    "model = ...\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, validation_data=(test_df['Content'], test_df['Label']), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61803271-fe03-4de8-9b58-d813ac12c683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
